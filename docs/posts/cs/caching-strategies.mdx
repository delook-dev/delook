---
title: '캐싱 전략(Caching Strategies)'
type: 'concept'
language: 'CS'
tags:
  - Caching
  - Performance Optimization
  - System Design
dateModified: 2025.09.14
---

# 캐싱 전략 (Caching Strategies)

---

## 1. 캐싱이란?

캐싱(Cache)이란 **자주 접근되는 데이터를 임시 저장소(메모리, 디스크 등)에 저장하여 빠르게 접근할 수 있도록 하는 기술**입니다.  
캐싱을 활용하면 원본 데이터 저장소(DB, 파일 시스템, 외부 API 등)에 매번 접근하지 않아도 되므로 성능과 확장성이 크게 향상됩니다.
다만, 캐시를 잘못 사용하면 원본과의 데이터 불일치로 인한 일관성 문제와 캐시 만료/오버플로우로 인한 성능 저하, 운영 복잡성 증가 등의 문제가 발생할 수 있습니다.

---

## 2. 캐싱 기본 개념

### 2-1) Cache Hit / Miss

- **Cache Hit**: 요청한 데이터가 캐시에 존재하여 즉시 응답
- **Cache Miss**: 캐시에 없어서 원본 저장소에서 데이터를 가져와야 하는 경우

### 2-2) Hit Rate & Miss Penalty

- **Hit Rate** = (Cache Hit 수 / 전체 요청 수)
- **Miss Penalty** = 캐시에 없을 때 원본에서 데이터를 가져오는 데 소요되는 추가 비용

### 2-3) Cold Cache & Cache Warm-up

- **Cold Cache**: 캐시가 비어 있어 모든 요청이 Miss 되는 초기 상태
- **Cache Warm-up**: 자주 사용될 데이터를 미리 캐시에 적재하여 Cold Start 비용을 줄이는 기법

### 2-4) Negative Caching

- 존재하지 않는 데이터 조회 결과(예: 없는 사용자 ID)도 캐싱하여 **불필요한 반복 쿼리**를 방지

---

## 3. 대표적인 캐싱 전략

### 3-1) Cache Aside (Lazy Loading)

- 애플리케이션이 캐시를 직접 관리하는 방식
- 흐름:
  1. 캐시에서 데이터 조회
  2. 없으면 원본에서 가져와 캐시에 저장
  3. 이후 요청은 캐시에서 응답

**장점**: 단순, 필요한 데이터만 캐싱함으로써 메모리 절약할 수 있습니다.  
**단점**: 첫 요청 시 Cache Miss가 발생합니다.

### 3-2) Read Through

- 캐시가 원본 저장소와 직접 연결
- 애플리케이션은 항상 캐시를 조회함으로써 캐시가 없으면 원본에서 가져와 자동 저장

**장점**: 애플리케이션 코드 단순화할 수 있습니다.  
**단점**: 캐시 계층에 더 많은 책임이 부여됩니다.

### 3-3) Write Through

- 데이터 쓰기 시 **캐시와 원본 저장소 모두에 기록**
- 읽기는 캐시에서 수행

**장점**: 캐시와 원본 데이터 간 일관성 보장할 수 있습니다.  
**단점**: 쓰기 성능이 저하될 수 있습니다.

### 3-4) Write Behind (Write Back)

- 쓰기를 캐시에만 기록하고, 일정 주기 후 비동기로 원본에 반영

**장점**: 쓰기 성능 향상할 수 있습니다.  
**단점**: 장애 발생 시 데이터 손실 위험이 있습니다.

---

## 4. 캐시 관리 기법

### 4-1) TTL (Time To Live)

- 캐시 데이터의 유효 기간 설정
- 오래된 데이터 자동 삭제함으로써 최신성 유지

### 4-2) Eviction Policy (교체 정책)

- 캐시 공간이 부족할 때 제거할 데이터를 선택하는 규칙
  - **LRU (Least Recently Used)**: 가장 오래 사용되지 않은 데이터 제거
  - **LFU (Least Frequently Used)**: 가장 적게 사용된 데이터 제거
  - **FIFO (First In First Out)**: 가장 먼저 들어온 데이터 제거

### 4-3) Preloading / Refresh Ahead

- TTL 만료 전에 데이터를 갱신하거나, 예상되는 데이터를 미리 로드

---

## 5. 캐싱 운영 시 발생하는 문제들

### 5-1) Cache Stampede (캐시 스탬피드)

- 다수의 요청이 동시에 Cache Miss가 발생함으로써 DB나 원본에 폭주하는 현상을 말합니다.
- **해결책**:
  - Mutex Lock / Single Flight
    - 첫 번째 요청만 원본 데이터를 가져오도록 잠금(Mutex)을 걸고, 나머지 요청은 대기
    - 원본 서버로 불필요한 중복 요청을 방지
  - Request Coalescing
    - 동시에 들어오는 여러 요청을 하나로 합쳐 처리
    - 처리 완료 후 결과를 모든 요청에 공유
  - Randomized TTL (Jitter)
    - 랜덤한 TTL을 설정하여 캐시 데이터의 만료 시간을 분산
    - 부하 분산과 캐시 서버 장애 방지

### 5-2) Cache Invalidation (무효화 전략)

- 원본 데이터 변경 시 캐시와 원본이 일치하지 않아 캐시를 무효화 시켜야합니다.
- 방법:
  - **Write Through**: 쓰기 시 자동 갱신
  - **Write Around**: DB만 갱신함으로써 캐시는 TTL로 만료
  - **Manual Invalidation**: 애플리케이션에서 직접 삭제

### 5-3) Consistency Models (일관성 모델)

- 캐시와 원본 간 일관성 수준
  - **Strong Consistency**: 항상 최신 데이터 제공 (성능 ↓)
  - **Eventual Consistency**: 일정 시간 후 동기화 (성능 ↑)

### 5-4) Hotspot & Thundering Herd

- 인기 데이터(Hot Key)에 요청이 몰려 캐시 서버·DB에 부하
- **해결책**: 분산 캐싱, 데이터 샤딩, Replica Cache 운영

### 5-5) Write Amplification (쓰기 증폭)

- Write Behind 시 작은 쓰기들이 모여 DB에 대량 기록함으로써 워크로드 패턴 변경
- SSD·DB 부하 고려 필요

---

## 6. 다양한 영역에서의 캐싱 활용

### 6-1) 운영체제(OS) 캐시

- 페이지 캐시(Page Cache): 디스크 I/O 속도 향상을 위해 최근 읽은 디스크 블록을 메모리에 저장
- 버퍼 캐시(Buffer Cache): 파일 시스템과 디스크 간 데이터 버퍼링

### 6-2) 데이터베이스 캐시

- 쿼리 결과 캐싱: 동일 쿼리 반복 시 DB 부하 감소
- 인덱스/버퍼 캐시: 자주 조회되는 인덱스나 테이블 데이터를 메모리에 유지

### 6-3) 웹/HTTP 캐시

- 브라우저 캐시: GET 요청 결과를 클라이언트에 저장함으로써 재요청 시 네트워크 비용 절감
- HTTP 헤더 기반 캐싱: Cache-Control, ETag, Last-Modified, Expires 활용
- 조건부 요청(Conditional Request): 서버에 변경 여부만 확인함으로써 효율적 데이터 전달

### 6-4) CDN(Content Delivery Network)

- 전 세계 분산 캐시 서버를 통해 사용자 가까이에서 콘텐츠 제공
- 정적 리소스 이미지, 동영상, JS/CSS 파일 등 빠른 전송
- TTL, Invalidation, Partial Caching 전략 활용

### 6-5) 프록시/리버스 프록시 캐시

- ISP, 기업 네트워크 등에서 반복 요청 캐시
- Nginx, Varnish, Squid 등 사용
- 웹 서버 부하 감소 및 대역폭 절약

### 6-6) 애플리케이션/인메모리 캐시

- Redis, Memcached 등 메모리 기반 캐시
- 세션 데이터, API 응답, 자주 조회되는 DB 데이터 캐싱

---

## 7. 캐싱 설계 시 고려사항

캐시를 설계할 때는 단순히 속도만 고려하는 것이 아니라, 데이터 특성과 운영 환경을 함께 평가해야 합니다.

- 데이터 일관성 vs 성능 트레이드오프
  캐시를 많이 활용할수록 성능은 향상되지만, 원본과의 데이터 불일치 위험이 증가합니다.
  따라서 시스템 특성에 맞추어 Strong Consistency와 Eventual Consistency 중 어느 수준의 일관성을 보장할 것인지 결정해야 합니다.
- 데이터 특성
  - 읽기 비율(Read-heavy): 읽기 요청이 많을수록 캐시 효율이 높습니다.
  - 데이터 크기(Size):  
    크기가 작고 자주 접근되는 데이터가 일반적으로 캐시에 적합합니다.  
    그러나 데이터 크기가 크더라도 네트워크 전송 비용이 크거나 반복 조회가 잦은 경우(예: 이미지, 동영상 썸네일, 대형 API 응답)에는 캐싱이 효과적일 수 있습니다.  
    반대로 크고 사용 빈도가 낮은 데이터는 캐시 효율을 떨어뜨리고 메모리 낭비를 유발할 수 있습니다.
  - 변경 빈도(Write Frequency): 변경이 적을수록 캐시 효율이 높습니다. 자주 변경되는 데이터는 무효화 전략을 함께 고려해야 합니다.
- 장애 대응(Fallback)
  캐시 서버 장애 시에도 원본 요청을 안정적으로 처리할 수 있는 전략이 필요합니다.
  Thundering Herd 방지, Retry 정책, Circuit Breaker 등을 고려하여 장애 시 서비스 연속성을 확보해야 합니다.
- 비용 고려
  메모리 사용량, 네트워크 트래픽, 운영 복잡성 등 다양한 요소를 종합적으로 평가해야 합니다.
  캐시는 성능을 크게 개선할 수 있지만, 무분별한 사용은 오히려 비용 증가와 관리 부담으로 이어질 수 있습니다.

---

## 8. 결론

- 캐싱은 시스템 성능과 확장성을 높이는 핵심 기술입니다.
- Cache Aside, Read/Write Through, Write Behind 전략을 상황에 맞게 선택해야 합니다.
- TTL, Eviction Policy, Invalidation, Stampede 대응 등 **운영 이슈까지 고려한 설계**가 필요합니다.
- 궁극적으로는 **성능 최적화와 데이터 일관성 사이의 균형**이 성공적인 캐싱 전략의 핵심입니다.
